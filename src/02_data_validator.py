# ============================================================
# Synthetic Dataset Validation Script — v1
#
# PURPOSE:
#   Validate the synthetic_game_sales_timeseries.csv dataset
#   generated by the synthetic universe. Checks include:
#       • Per-title sanity checks (dates, duplicates, NaNs, anomalies)
#       • Global dataset checks
#       • Health score assignment
#       • Consolidated text + JSON reporting
#
# INPUT:
#   synthetic_data/synthetic_game_sales_timeseries.csv
#
# OUTPUT:
#   validation/
#       validation_title_<id>.txt / .json
#       global_validation.txt / .json
#       consolidated_summary.txt / .json
# RUN:
#   02_data_validator.py
#
# ============================================================


# ============================================================
# IMPORTS
# ============================================================

import os
import json
import numpy as np
import pandas as pd


# ============================================================
# GLOBAL CONFIG
# ============================================================

DATA_PATH = "synthetic_data/synthetic_game_sales_timeseries.csv"
OUT_DIR = "validation"
os.makedirs(OUT_DIR, exist_ok=True)

EPS = 1e-6


# ============================================================
# HELPER UTILITIES
# ============================================================

def add_issue(issues, level, msg):
    """
    Append a validation issue to a given list.

    Args:
        issues (list): collector list
        level (str): "CRITICAL" or "WARNING"
        msg (str): human-readable message
    """
    issues.append({"level": level, "message": msg})


def rolling_z(series, window=13):
    """
    Compute rolling z-score for anomaly detection.

    High absolute values (>6) indicate potential sales spikes or issues.
    """
    s = series.astype(float)
    mu = s.rolling(window).mean()
    sd = s.rolling(window).std()
    return (s - mu) / (sd + EPS)


# ============================================================
# PER-TITLE VALIDATION
# ============================================================

def validate_title(df_title, title_id):
    """
    Validate a single title's time series.

    Checks:
        - Required columns
        - Duplicate dates
        - Negative sales
        - Extreme z-score anomalies
        - Marketing sanity
        - NaN flood percentage
    """
    issues = []

    # Basic required fields
    required = ["week_start_date", "sales"]
    for col in required:
        if col not in df_title.columns:
            add_issue(issues, "CRITICAL", "Missing column: " + col)
            return issues

    # Date parsing
    try:
        df_title["week_start_date"] = pd.to_datetime(df_title["week_start_date"], errors="coerce")
    except Exception:
        add_issue(issues, "CRITICAL", "Invalid dates in week_start_date")
        return issues

    if df_title["week_start_date"].isna().any():
        add_issue(issues, "CRITICAL", "Invalid date values detected")
        return issues

    df_title = df_title.sort_values("week_start_date")

    # --------------------------------------------------------
    # 1. Duplicate dates
    # --------------------------------------------------------

    if df_title["week_start_date"].duplicated().any():
        add_issue(issues, "CRITICAL", "Duplicate weekly dates detected")

    # --------------------------------------------------------
    # 2. Negative sales
    # --------------------------------------------------------

    if (df_title["sales"] < 0).any():
        add_issue(issues, "CRITICAL", "Negative sales detected")

    # --------------------------------------------------------
    # 3. Sales anomaly z-score
    # --------------------------------------------------------

    z = rolling_z(df_title["sales"])
    if (z.abs() > 6).any():
        add_issue(issues, "WARNING", "Sales z-scores exceed 6")

    # --------------------------------------------------------
    # 4. Marketing sanity
    # --------------------------------------------------------

    if "marketing_index" in df_title.columns:
        if (df_title["marketing_index"] < 0).any():
            add_issue(issues, "WARNING", "Negative marketing_index")

    # --------------------------------------------------------
    # 5. NaN flood check
    # --------------------------------------------------------

    nan_frac = df_title.isna().mean()
    flooded = nan_frac[nan_frac > 0.30]
    for col, frac in flooded.items():
        add_issue(issues, "WARNING", col + " is " + str(round(frac * 100, 1)) + " percent NaN")

    return issues


# ============================================================
# GLOBAL VALIDATION
# ============================================================

def validate_global(df):
    """
    Apply global dataset checks across all titles:

        - Required columns
        - Duplicate (title_id, week_start_date)
        - Misaligned histories (end-of-series date gaps)
        - Global NaN flooding
    """
    issues = []

    required = ["title_id", "week_start_date", "sales"]
    for col in required:
        if col not in df.columns:
            add_issue(issues, "CRITICAL", "Global missing column: " + col)
            return issues

    df["week_start_date"] = pd.to_datetime(df["week_start_date"], errors="coerce")

    if df["week_start_date"].isna().any():
        add_issue(issues, "CRITICAL", "Invalid dates in global dataset")
        return issues

    # --------------------------------------------------------
    # 1. Duplicate rows
    # --------------------------------------------------------

    if df.duplicated(subset=["title_id", "week_start_date"]).any():
        add_issue(issues, "CRITICAL", "Duplicate (title_id, week_start_date) rows")

    # --------------------------------------------------------
    # 2. Misaligned histories (vol-arb rule)
    # --------------------------------------------------------

    ranges = df.groupby("title_id")["week_start_date"].agg(["min", "max"])
    max_end = ranges["max"].max()
    cutoff = max_end - pd.Timedelta(days=7)

    misaligned = ranges[ranges["max"] < cutoff]
    if len(misaligned) > 0:
        add_issue(
            issues,
            "WARNING",
            "Titles ending more than 7 days earlier: " + str(list(misaligned.index))
        )

    # --------------------------------------------------------
    # 3. Global NaN flood
    # --------------------------------------------------------

    nan_frac = df.isna().mean()
    flooded = nan_frac[nan_frac > 0.40]
    for col, frac in flooded.items():
        add_issue(
            issues,
            "WARNING",
            "Column " + col + " is " + str(round(frac * 100, 1)) + " percent NaN"
        )

    return issues


# ============================================================
# MAIN EXECUTION
# ============================================================

if __name__ == "__main__":

    print("Loading dataset:", DATA_PATH)

    if not os.path.exists(DATA_PATH):
        print("ERROR: dataset not found:", DATA_PATH)
        raise SystemExit

    df = pd.read_csv(DATA_PATH)

    # --------------------------------------------------------
    # PER-TITLE VALIDATION LOOP
    # --------------------------------------------------------

    global_summary = {}
    for title_id, df_t in df.groupby("title_id"):

        issues = validate_title(df_t.copy(), title_id)
        global_summary[title_id] = issues

        # Write individual reports
        txt = os.path.join(OUT_DIR, f"validation_title_{title_id}.txt")
        jsn = os.path.join(OUT_DIR, f"validation_title_{title_id}.json")

        with open(txt, "w", encoding="ascii", errors="ignore") as f:
            if not issues:
                f.write("CLEAN\n")
            else:
                for item in issues:
                    f.write("[" + item["level"] + "] " + item["message"] + "\n")

        with open(jsn, "w", encoding="ascii", errors="ignore") as f:
            json.dump({"title_id": int(title_id), "issues": issues}, f, indent=2)

    # --------------------------------------------------------
    # GLOBAL VALIDATION
    # --------------------------------------------------------

    global_issues = validate_global(df.copy())

    txt = os.path.join(OUT_DIR, "global_validation.txt")
    with open(txt, "w", encoding="ascii", errors="ignore") as f:
        if not global_issues:
            f.write("GLOBAL CLEAN\n")
        else:
            for item in global_issues:
                f.write("[" + item["level"] + "] " + item["message"] + "\n")

    jsn = os.path.join(OUT_DIR, "global_validation.json")
    with open(jsn, "w", encoding="ascii", errors="ignore") as f:
        json.dump({"issues": global_issues}, f, indent=2)

    # --------------------------------------------------------
    # HEALTH SCORES
    # --------------------------------------------------------

    health_scores = {}
    for title_id, issues in global_summary.items():
        if not issues:
            score = 100
        else:
            crit = sum(1 for x in issues if x["level"] == "CRITICAL")
            warn = sum(1 for x in issues if x["level"] == "WARNING")
            score = max(0, min(100, 100 - 40 * crit - 10 * warn))
        health_scores[title_id] = score

    # --------------------------------------------------------
    # CONSOLIDATED SUMMARY
    # --------------------------------------------------------

    consolidated = {}
    for title_id, issues in global_summary.items():
        if not issues:
            consolidated[title_id] = ["CLEAN"]
        else:
            consolidated[title_id] = [x["level"] + ": " + x["message"] for x in issues]

    # TXT summary
    summary_txt = os.path.join(OUT_DIR, "consolidated_summary.txt")
    with open(summary_txt, "w", encoding="ascii", errors="ignore") as f:
        for title_id, msgs in consolidated.items():
            score = health_scores[title_id]
            if msgs == ["CLEAN"]:
                f.write(str(title_id) + ": CLEAN | Health: " + str(score) + "\n")
            else:
                f.write(str(title_id) + ": " + "; ".join(msgs) + " | Health: " + str(score) + "\n")

    # JSON summary
    summary_json = os.path.join(OUT_DIR, "consolidated_summary.json")
    with open(summary_json, "w", encoding="ascii", errors="ignore") as f:
        json.dump(
            {
                "titles": consolidated,
                "health_scores": health_scores,
                "global_issues": global_issues
            },
            f,
            indent=2
        )

    print("Validation complete.")
